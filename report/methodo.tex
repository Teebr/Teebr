%!TEX root = rapport.tex

\section{Méthodes}

\subsection{Technologies}

Le projet est réalisé intégralement en Python, et est divisé en trois grandes
parties.

La première est la récupération des données en utilisant l’\api{} de \twt{}
avec la bibliothèque \verb|tweepy| et \verb|pybayesantispam| pour filtrer les
tweets.

La deuxième est le traitement et stockage des données, à l’aide des
bibliothèques \verb|unidecode|, \verb|nltk| et \verb|textblob| pour le
traitement et une base de données SQLite pour le stockage. Le code utilise la
bibliothèque \verb|peewee| qui sert d’ORM\footnote{Object-Relational Mapping}
pour la base. Cette abstraction permet de ne pas dépendre du format de la base
de données. Il serait possible d’utiliser une base PostgreSQL sans changer le
code. L’utilisation d’SQLite permet de pouvoir tester rapidemment sans devoir
lancer de serveur de base de données ainsi que de profiter d’un schéma strict,
que ne peuvent pas offrir des bases NoSQL comme MongoDB ou ElasticSearch.
L’utilisation d’une base plutôt qu’un ensemble de fichiers JSON permet de faire
des recherches beaucoup plus performantes.

Enfin, la troisième est l’interface Web, à l’aide des bibliothèques
\verb|Flask| et \verb|Gunicorn| pour la gestion des requêtes et de \verb|Babel|
pour la localisation. L’interface utilise également \verb|LESS| pour
pré-processer le CSS, le framework \verb|Bootstrap| pour une base de mise en
forme et \verb|AngularJS| pour l’interaction sur les pages. Le site Web tourne
dans un conteneur \I{Docker}, et est déployé avec \verb|dokku|.

L’ensemble du code, y compris les commentaires, est écrit en anglais, et le
projet est versionné avec Git.

De façon générale, toutes les bibliothèques externes dont dépend le code Python
sont listées dans le fichier \verb|requirements.txt|, à la racine du
répertoire.

\subsection{Données}

Les données sont des \tweets{} qui proviennent directement de \twt{}, via son
\api{}. Elles ne sont pas fixées, et le projet est conçu pour fonctionner avec
n’importe quel ensemble de \tweets{}, qui sont importés à l’aide d’un script.

\subsubsection{Récupération}

La récupération des données se fait en deux temps. Le premier, qui était
originellement le seul, est la \I{découverte}. On utilise ici l’\api{}
\I{Streaming} de \twt{}, qui permet de se brancher sur un flux temps-réel qui
nous permet de récupérer un échantillon de tout ce qui est produit sur la
plateforme. Il est possible de filtrer les \tweets{} par mot-clefs et comptes
producteurs.

On utilise ici une liste d’environ 170 termes renseignés à la main pour couvrir
le thème des nouvelles technologies. Elle comprend notamment la liste des
langages de programmation les plus utilisés sur
GitHub\footnote{\url{http://githut.info/}} ainsi qu’une liste d’entreprises
comme Google, Amazon et Apple. Afin de ne pas manquer de \tweets{} avec une
liste fermée comme celle-ci, on ajoute une liste d’environ 140 comptes à
suivre, compilée depuis plusieurs sources sur le Web.

Les auteurs des \tweets{} sont également importés dans la base de données,
puisque c’est eux que l’on souhaite recommander. Toutes les informations
renvoyées par l’\api{} sont stockées, notamment les compteurs de \tweets{},
abonnés, favoris ainsi que les informations du profil telles que la biographie,
l’avatar ou le site Web.

Cette récupération en temps réel permet plus de \I{découvrir} des producteurs
potentiels que de réellement récolter du contenu. Celui-ci est très lié à
l’actualité en cours, et l’aspect court-terme de la récupération nous donne
beaucoup de producteurs, mais avec très peu de contenu par producteur car peu
d’entre eux auront le temps de tweeter deux fois pendant la fenêtre de
récupération\footnote{Pour donner un ordre d’idée, le flux nous permet
d’importer un peu plus de 1000 comptes par minute dans la base de données}.

Il est donc nécessaire de faire une récupération plus approfondie. On utilise
ici toujours l’\api{} de \twt{}, mais on parcourt cette fois-ci tous les
producteurs enregistrés dans la base grâce à la précédente passe et on récupère
leurs derniers \tweets{}\footnote{Au maximum 50 par compte actuellement, mais
ce montant est paramétrable}. Il
faut un nombre conséquent de \tweets{} pour générer la signature d’un
producteur et cette passe nous permet de le faire. Elle corrige également les
défauts de la précédente notamment la sensibilité à l’actualité du moment.

En plus des données fournies par l’\api{}, nous extrayons les entitées nommées
des \tweets{} à l’aide de la bibliothèque \verb|textblob|, basée sur
\verb|nltk|, avec un pré-nettoyage du texte maison et un post-traitement pour
supprimer les faux positifs\footnote{Voir dans \verb|teebr.text.utils|}. Ces
entitées sont stockées dans un champ de chaque \tweet{}. Il y a souvent des
faux positifs, mais cela réduit déjà les possibilités de représentation du
\tweet{} : il est plus facile d’agréger les tweets avec une courte liste de
mots plutôt qu’avec un texte de 140 caractères qui contient des URLs, des
smileys, et surtout des mots qui n’ajoutent pas d’information concernant le
sujet (\I{stopwords}).

\subsubsection{Sélection}

Bien que l’on puisse techniquement utiliser tout \tweet{}, il est nécessaire
ici d’effectuer une pré-sélection des \tweets{} pour réduire le travail de
classification.

On se limite ainsi aux \tweets{} écrits en anglais. On supprime les réponses
et les \rts{}\footnote{Voir \verb|teebr.features.filter_status|} ainsi que les
spams à l’aide d’un filtre Bayésien simple, entraîné à la main sur plus de 1500
\tweets{}\footnote{Voir \verb|scripts/train_spam.py|}.

Le spam a été un gros problème lors de ce projet, plus de la moitié des
\tweets{} récupérés étaient du spam au début. L’utilisation du filtre Bayésien
a considérablement réduit leur nombre sans toutefois les éliminer. Nous avons
dû utiliser des méthodes très agressives comme l’élimination automatique des
comptes dont un seul des \tweets{} est détecté comme du spam\footnote{Voir dans
\verb|teebr.maintenance|}.

\subsection{Traitement}

Une fois qu’un \tweet{} a passé tous les filtres et que nous disposons des noms
mentionnés dans son texte, il est inséré dans la base de données. La
catégorisation des \tweets{} étant le second plus gros problème rencontré dans
ce projet après le spam, nous avons simplifié le principe pour avoir une démo
fonctionnelle pour la soutenance.

L’objectif de la classification est de pouvoir définir un ensemble fini de
catégories pour l’ensemble du corpus, qui lui n’a pas de taille finie. Chaque
\tweet{} est donc classée dans une ou plusieurs catégories, chacune d’entre
elle correspondant à une caractéristique des signatures que l’on cherche à
extraire. La solution utilisée par manque de temps a été de regarder les termes
apparaissant le plus souvent dans les entités extraites des \tweets{} dans un
corpus de $25000$ messages et d’utiliser les 70 plus utilisés comme catégories.
Le fait que certains d’entre eux se recoupent n’est pas important car un
\tweet{} peut appartenir à plusieurs catégories.

Les données extraites de l’\api{} ainsi que les catégories nous donnent une
petite centaine de caractéristiques pour chaque entitée de la base (\tweet{},
producteur et consommateur).

Nous calculons la signature d’un \tweet{} en fonction de ses caractéristiques,
qui sont toutes booléennes. Si un \tweet{} est dans une catégorie, alors la
caractéristique correspondante sera positive, s’il contient une URL la
caractéristique « contient une URL » sera positive, etc. Nous ajoutons cette
signature à la signature du producteur du \tweet{}, par simple addition des
caractéristiques. Chaque producteur conserve un compteur de \tweets{} produit,
permettant de le comparer à un autre producteur ayant un nombre différent de
\tweets{} en calculant un ratio pour chaque caractéristique.

Nous proposons un ensemble aléatoire de \tweets{} aux utilisateurs, qui peuvent
les noter. Une note positive ajoute la signature du \tweet{} à celle de
l’utilisateur, de façon identique à ce qu’on fait pour les producteurs. Une
note négative n’a pas d’effet sur la signature du consommateur. Au début du
projet, nous retirions les signatures des \tweets{} rejetés de la signature du
consommateur mais le faible ratio
$(\text{\tweets{} acceptés} / \text{\tweets{} refusés})$ donnait des signatures
négatives tout le temps. Aussi, cela nous faisait perdre trop d’information sur
les \tweets{} acceptés.

La similarité entre producteurs et consommateurs se fait ensuite à partir de la
similarité de leurs signatures. On ne conserve que les caractéristiques qui
sont non-nulles pour chacun d’entre eux, les divisent par le nombre de
\tweets{} produits pour l’un et noté pour l’autre, et effectuons une somme
pondérée des différences\footnote{Voir \verb|teebr.recommendation|}. Cette
somme est ensuite divisée par la somme des pondération des caractéristiques
utilisées pour obtenir une distance entre $0$ et $1$. Une distance de $0$
signifie que les deux utilisateurs sont parfaitement proches et une distance
de $1$ signifie qu’elles sont très éloignées.

\begin{equation}
  \label{eq:sig_proc_cons}
  \begin{split}
    producer &= (\,43, 12, 26, 1, \ldots, 16\,)\quad\qquad\text{(43 tweets)} \\
             &\Rightarrow (\,1, 0.28, 0.6, 0.02, \ldots, 0.37\,) \\
    \\
    %                                         poor man's spacing
    consumer &= (\,17, 25, 3, 0, \ldots, 12\,)\,\,\,\quad\qquad\text{(25 tweets)} \\
             &\Rightarrow (\,0.68, 1, 0.12, 0, \ldots, 0.48\,)
  \end{split}
\end{equation}

Si on prend comme exemple les signatures de la figure \ref{eq:sig_proc_cons},
avec les pondérations $(3, 2, 1, 1, 2)$, on obtient la valeur de similarité en
calculant la différence entre chaque signature. Pour chaque indice $i$, on
calcule la différence pondérée entre les éléments correspondants :
$|\text{producer}_i - \text{consumer}_i|\times\text{pondération}_i$. Toutes les
différences sont ensuite additionnées et le total est divisé par la somme des
pondérations :

$$
\frac{|1-0.68|\times 3+|0.28-1|\times 2+|0.6-0.12|+|0.02-0|+|0.37-0.48|\times 2}{3 + 2 + 1 + 1 + 2}=0.35
$$

Le résultat est un nombre entre 0 et 1. Plus la distance est basse, plus les
profils sont proches. Dans notre exemple, la similarité est plutôt bonne, sans
être extraordinaire.
