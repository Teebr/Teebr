%!TEX root = rapport.tex

\section{Méthodes}

\subsection{Technologies}

Le projet est réalisé intégralement en Python, et est divisé en trois grandes
parties.

\begin{itemize}
  \item Récupération des données en utilisant l’\api{} Streaming de \twt{} avec
    la bibliothèque \verb|tweepy| et \verb|pybayesantispam| pour filtrer les
    tweets.
  \item Traitement et stockage des données, à l’aide des bibliothèques
    \verb|unidecode|, \verb|nltk| et \verb|textblob| pour le traitement et une
    base de données SQLite pour le stockage. Le code utilise la bibliothèque
    \verb|peewee| qui sert d’ORM pour la base. Cette abstraction permet de ne
    pas dépendre du format de la base de données, il serait possible d’utiliser
    une base PostgreSQL sans changer le code. L’utilisation d’SQLite permet de
    pouvoir tester rapidemment sans devoir lancer de serveur de base de
    données, ainsi que de profiter d’un schéma strict, que ne peuvent pas
    offrir des bases NoSQL comme MongoDB ou ElasticSearch. L’utilisation d’une
    base plutôt qu’un ensemble de fichiers JSON permet de faire des recherches
    beaucoup plus performantes.
  \item Interface Web, à l’aide des bibliothèques \verb|Flask| et
    \verb|Gunicorn| pour la gestion des requêtes et de \verb|Babel| pour la
    localisation (l10n). L’Interface utilise également \verb|LESS| pour
    pré-processer le CSS, et le framework \verb|Bootstrap| pour la mise en
    forme. Le site Web tourne dans un conteneur \I{Docker}, et est déployé avec
    \verb|dokku|.
\end{itemize}

L’ensemble du code, y compris les commentaires, est écrit en anglais, et le
projet est versionné avec Git.

\subsection{Données}

Les données sont des \tweets{} qui proviennent directement de \twt{}, via son
\api{}. Elles ne sont pas fixées, et le projet est conçu pour fonctionner avec
n’importe quel ensemble de \tweets{}, qui sont importés à l’aide d’un script.

\subsubsection{Sélection}

Le flux de \tweets{} récupéré depuis l’\api{} \twt{} doit être filtré pour être
intéressant. On se connecte à l’\api{} en envoyant une liste de mots-clefs
(\verb|keywords.txt|) à suivre, et celle-ci nous envoit un échantillon des
tweets qui mentionnent ceux-ci en temps réel. La liste de mots-clefs compte
environ 170 termes renseignés à la main pour couvrir le thème des nouvelles
technologies. Elle comprend notamment la liste des langages de programmation
les plus utilisés sur GitHub\footnote{\url{http://githut.info/}} ainsi qu’une
liste d’entreprises comme Google, Amazon et Apple. Afin de ne pas rater de
\tweets{} avec une liste fermée comme celle-ci, on ajoute une liste d’environ
140 comptes à suivre, compilée depuis
  \href{http://uk.businessinsider.com/100-best-tech-people-on-twitter-2014-2014-11?op=1?r=US}{un
    article de Business Insider}.

Chaque \tweet{} passe ensuite une série de filtres\footnote{Voir
\verb|teebr.features.filter_status|} pour éliminer les tweets en langue autre
que l’anglais, les réponses et \rts{} ainsi que les spams. Ce dernier filtre se
base sur un filtre Bayésien simple, entraîné à la main sur environ 300
\tweets{}\footnote{Voir \verb|scripts/train_spam.py|}.

Enfin, les auteurs des \tweets{} sont également importés dans la base de
données, puisque c’est eux que l’on souhaite recommander. Toutes les
informations renvoyées par l’\api{} sont stockées, notamment les compteurs de
\tweets{}, abonnés, favoris ainsi que les informations du profil tel que la
biographie, l’avatar ou le site Web.

\subsection{Traitement}

TODO
